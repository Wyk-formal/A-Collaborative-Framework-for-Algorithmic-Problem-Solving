# -*- coding: utf-8 -*-
"""
æœç´¢å¼•æ“ - å®Œå…¨ä¿æŒåŸæœ‰åŠŸèƒ½
ä»main.pyä¸­æå–çš„æœç´¢ç›¸å…³å‡½æ•°ï¼ŒåŠŸèƒ½å®Œå…¨ä¸å˜
"""

import numpy as np
from zai import ZhipuAiClient
from neo4j import GraphDatabase
from ..config.settings import settings

# ä¿æŒåŸæœ‰çš„å…¨å±€å˜é‡åˆå§‹åŒ–
zhipu = ZhipuAiClient(api_key=settings.ai.zhipu_api_key)
driver = GraphDatabase.driver(settings.database.neo4j_uri, auth=(settings.database.neo4j_user, settings.database.neo4j_password))

# å¦‚æœä¸å¸Œæœ›çœ‹åˆ°è­¦å‘Šï¼Œå¯ä»¥è¿‡æ»¤è­¦å‘Šä¿¡æ¯
if not settings.system.show_query_warnings:
    import warnings
    warnings.filterwarnings("ignore", category=DeprecationWarning, module="neo4j")

def debug_print(*args, **kwargs):
    """å—æ§çš„è°ƒè¯•è¾“å‡ºå‡½æ•° - ä¿æŒåŸæœ‰åŠŸèƒ½"""
    if settings.system.show_debug_info:
        print(*args, **kwargs)

def l2_normalize(vec):
    """L2æ ‡å‡†åŒ– - ä¿æŒåŸæœ‰åŠŸèƒ½"""
    v = np.array(vec, dtype=np.float32)
    n = np.linalg.norm(v) + 1e-12
    return (v / n).tolist()

def embed_query_with_zhipu(text: str):
    """ä½¿ç”¨æ™ºè°±AIè¿›è¡Œæ–‡æœ¬åµŒå…¥ - ä¿æŒåŸæœ‰åŠŸèƒ½"""
    resp = zhipu.embeddings.create(model=settings.ai.embedding_model, input=text)
    vec = resp.data[0].embedding
    return l2_normalize(vec)

def extract_algorithm_keywords(question: str):
    """æå–ç®—æ³•å…³é”®è¯ - ä¿æŒåŸæœ‰åŠŸèƒ½"""
    algorithm_keywords = [
        "åŠ¨æ€è§„åˆ’", "dp", "è´ªå¿ƒ", "åˆ†æ²»", "äºŒåˆ†", "åŒæŒ‡é’ˆ", "æ»‘åŠ¨çª—å£", "å‰ç¼€å’Œ", "å·®åˆ†",
        "çº¿æ®µæ ‘", "æ ‘çŠ¶æ•°ç»„", "å¹¶æŸ¥é›†", "æœ€çŸ­è·¯", "æœ€å°ç”Ÿæˆæ ‘", "æ‹“æ‰‘æ’åº", "å¼ºè¿é€šåˆ†é‡",
        "ç½‘ç»œæµ", "æœ€å¤§æµ", "æœ€å°å‰²", "äºŒåˆ†å›¾", "åŒ¹é…", "åŒˆç‰™åˆ©", "KMç®—æ³•", "è´¹ç”¨æµ",
        "è«é˜Ÿ", "åˆ†å—", "ä¸»å¸­æ ‘", "å¯æŒä¹…åŒ–", "å¹³è¡¡æ ‘", "çº¢é»‘æ ‘", "AVL", "Treap",
        "å“ˆå¸Œ", "KMP", "ACè‡ªåŠ¨æœº", "åç¼€æ•°ç»„", "åç¼€è‡ªåŠ¨æœº", "å›æ–‡æ ‘", "Manacher",
        "FFT", "NTT", "å¿«é€Ÿå¹‚", "çŸ©é˜µå¿«é€Ÿå¹‚", "é«˜æ–¯æ¶ˆå…ƒ", "çº¿æ€§åŸº", "å®¹æ–¥åŸç†",
        "ç»„åˆæ•°å­¦", "æ•°è®º", "æ¬§æ‹‰å‡½æ•°", "è«æ¯”ä¹Œæ–¯", "æœæ•™ç­›", "min25ç­›", "æ´²é˜ç­›",
        "å‡ ä½•", "å‡¸åŒ…", "æ—‹è½¬å¡å£³", "åŠå¹³é¢äº¤", "åœ†", "å¤šè¾¹å½¢", "æ‰«æçº¿", "CDQåˆ†æ²»",
        "æ•´ä½“äºŒåˆ†", "ç¦»çº¿", "åœ¨çº¿", "å¼ºåˆ¶åœ¨çº¿", "å¯æŒä¹…åŒ–", "å›æ»š", "æ’¤é”€"
    ]
    
    found_keywords = []
    question_lower = question.lower()
    for keyword in algorithm_keywords:
        if keyword.lower() in question_lower:
            found_keywords.append(keyword)
    
    return found_keywords

def clean_query_text(text: str):
    """æ¸…ç†æŸ¥è¯¢æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½å¯¼è‡´Luceneè§£æé”™è¯¯çš„ç‰¹æ®Šå­—ç¬¦ - ä¿æŒåŸæœ‰åŠŸèƒ½"""
    if not text or not text.strip():
        return "ç®—æ³• é¢˜ç›®"
    
    # ç§»é™¤æˆ–æ›¿æ¢Luceneç‰¹æ®Šå­—ç¬¦å’Œå¯èƒ½å¯¼è‡´é—®é¢˜çš„ç¬¦å·
    special_chars = [
        '[', ']', '(', ')', '{', '}', '~', '^', '"', '*', '?', '\\', 
        ':', '+', '-', '!', '/', '|', '&', '<', '>', '=', '@', '#',
        '$', '%', 'ã€‚', 'ï¼Œ', 'ï¼›', 'ï¼š', 'ï¼', 'ï¼Ÿ', 'ã€', 'ã€Š', 'ã€‹',
        '"', '"', ''', ''', 'ã€', 'ã€‘', 'ï¼ˆ', 'ï¼‰', 'Â·', 'â€¦', 'â€”',
        '`', "'", '\n', '\r', '\t'  # æ–°å¢ä¸€äº›å¯èƒ½å¯¼è‡´é—®é¢˜çš„å­—ç¬¦
    ]
    
    cleaned = text
    for char in special_chars:
        cleaned = cleaned.replace(char, ' ')
    
    # ç§»é™¤è¿ç»­çš„ç©ºæ ¼ï¼Œä¿ç•™å•ä¸ªç©ºæ ¼
    cleaned = ' '.join(cleaned.split())
    
    # ç§»é™¤å‰åç©ºæ ¼
    cleaned = cleaned.strip()
    
    # å¦‚æœæ¸…ç†åçš„æ–‡æœ¬å¤ªçŸ­æˆ–ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æœç´¢è¯
    if len(cleaned) < 2:
        cleaned = "ç®—æ³• é¢˜ç›®"
    
    # é™åˆ¶æŸ¥è¯¢é•¿åº¦ï¼Œé¿å…è¿‡é•¿çš„æŸ¥è¯¢å¯¼è‡´é—®é¢˜
    if len(cleaned) > 200:
        cleaned = cleaned[:200].strip()
    
    # ç¡®ä¿ä¸ä»¥ç‰¹æ®Šå­—ç¬¦ç»“å°¾ï¼Œè¿™å¯èƒ½å¯¼è‡´Luceneè§£æé—®é¢˜
    while cleaned and cleaned[-1] in '+-&|!(){}[]^"~*?:\\':
        cleaned = cleaned[:-1].strip()
    
    # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›é»˜è®¤å€¼
    if not cleaned:
        cleaned = "ç®—æ³• é¢˜ç›®"
    
    return cleaned

# å¢å¼ºçš„æ··åˆæ£€ç´¢CypheræŸ¥è¯¢ - ä¿æŒåŸæœ‰åŠŸèƒ½
CYPHER_ENHANCED_HYBRID = """
// å‚æ•°ï¼š$q (string), $qvec (list<float>), $keywords (list<string>)

// A. å…¨æ–‡å€™é€‰ï¼ˆåŠ æƒ 0.3ï¼‰
CALL ($q) {
  WITH $q AS q
  CALL db.index.fulltext.queryNodes('alg_fulltext', q) YIELD node, score
  RETURN collect({a: node, s: score * 0.3, route:'fulltext'}) AS T
}

// B. å‘é‡å€™é€‰ï¼ˆåŠ æƒ 0.5ï¼‰
CALL ($qvec) {
  WITH $qvec AS qv
  CALL db.index.vector.queryNodes('chunk_vec_idx', 15, qv) YIELD node, score
  MATCH (node)<-[:HAS_CHUNK]-(alg:Algorithm)
  RETURN collect({a: alg, s: score * 0.5, route:'vector'}) AS V
}

// C. å…³é”®è¯åŒ¹é…å€™é€‰ï¼ˆåŠ æƒ 0.2ï¼‰
CALL ($keywords) {
  WITH $keywords AS kw
  UNWIND kw AS keyword
  MATCH (a:Algorithm)
  WHERE any(alias IN a.aliases WHERE toLower(alias) CONTAINS toLower(keyword))
     OR any(k IN a.keywords WHERE toLower(k) CONTAINS toLower(keyword))
     OR toLower(a.title) CONTAINS toLower(keyword)
  RETURN collect({a: a, s: 0.2, route:'keyword'}) AS K
}

// åˆå¹¶ä¸‰è·¯å€™é€‰å¹¶æŒ‰ç®—æ³•èšåˆåˆ†æ•°
WITH T + V + K AS R
UNWIND R AS r
WITH r.a AS a, collect(r) AS contribs
WITH a, reduce(s=0.0, x IN contribs | s + x.s) AS fused, contribs
ORDER BY fused DESC
LIMIT 8

// å›¾æ‰©å±• + å®Œæ•´ä¿¡æ¯ï¼ˆå¢å¼ºç¤ºä¾‹ä»£ç è·å–ï¼‰
OPTIONAL MATCH (a)-[:PREREQUISITE]->(p:Concept)
OPTIONAL MATCH (a)-[:APPLICATION]->(u:Concept)
OPTIONAL MATCH (a)-[:PITFALL]->(f:Concept)
OPTIONAL MATCH (a)-[:HAS_EXAMPLE]->(e:Example)
OPTIONAL MATCH (a)-[:HAS_CHUNK]->(c:Chunk)

// è·å–è¯¦ç»†çš„ç¤ºä¾‹ä»£ç ä¿¡æ¯
WITH a, fused, contribs,
     collect(DISTINCT p.name)[0..8] AS prereq,
     collect(DISTINCT u.name)[0..8] AS apps,
     collect(DISTINCT f.name)[0..8] AS pitfalls,
     collect(DISTINCT e)[0..8] AS all_examples,
     [c IN collect(DISTINCT c)[0..5] | c.content] AS snippets

// æ„å»ºè¯¦ç»†çš„ç¤ºä¾‹ä¿¡æ¯ï¼ŒåªåŒ…å«ç¡®å®å­˜åœ¨çš„å±æ€§
WITH a, fused, contribs, prereq, apps, pitfalls, snippets,
     [ex IN all_examples | {
         id: ex.id,
         title: ex.title,
         description: ex.description,
         code: ex.code,
         solution: ex.solution
     }] AS detailed_examples

RETURN a.uid AS uid, a.title AS title,
       a.principle AS principle, a.cpx_time AS time, a.cpx_space AS space,
       a.intro AS intro, a.keywords AS keywords, a.aliases AS aliases,
       prereq, apps, pitfalls, detailed_examples, snippets, fused, contribs;
"""

def enhanced_hybrid_search(question: str, keywords_hint: str = ""):
    """å¢å¼ºçš„æ··åˆæ£€ç´¢ï¼Œæ”¯æŒAIæå–çš„å…³é”®è¯æç¤º - ä¿æŒåŸæœ‰åŠŸèƒ½"""
    # ä½¿ç”¨AIæ€»ç»“çš„å†…å®¹è¿›è¡ŒæŸ¥è¯¢ï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦é—®é¢˜
    if keywords_hint:
        # å¦‚æœæœ‰AIæå–çš„å…³é”®è¯ï¼Œä¼˜å…ˆä½¿ç”¨è¿™äº›å…³é”®è¯
        search_text = keywords_hint
        debug_print(f"ã€ä½¿ç”¨AIæå–çš„å…³é”®è¯ã€‘{keywords_hint}")
    else:
        # å¦‚æœæ²¡æœ‰å…³é”®è¯æç¤ºï¼Œä½¿ç”¨åŸæ¥çš„æ¸…ç†æ–¹æ³•
        search_text = clean_query_text(question)
        debug_print(f"ã€åŸå§‹æŸ¥è¯¢ã€‘{question[:50]}...")
        debug_print(f"ã€æ¸…ç†åæŸ¥è¯¢ã€‘{search_text}")
    
    qvec = embed_query_with_zhipu(question)
    # ç»“åˆåŸå§‹å…³é”®è¯æå–å’ŒAIæå–çš„å…³é”®è¯
    extracted_keywords = extract_algorithm_keywords(question)
    if keywords_hint:
        # æ·»åŠ AIæå–çš„å…³é”®è¯ï¼ˆä¿æŒæƒé‡é¡ºåºï¼‰
        ai_keywords = [kw.strip() for kw in keywords_hint.split(',') if kw.strip()]
        # ä½¿ç”¨æœ‰åºå»é‡ï¼Œä¿æŒAIå…³é”®è¯çš„æƒé‡é¡ºåºåœ¨å‰
        seen = set()
        merged_keywords = []
        # é¦–å…ˆæ·»åŠ AIå…³é”®è¯ï¼ˆæŒ‰æƒé‡æ’åºï¼‰
        for kw in ai_keywords:
            if kw not in seen:
                merged_keywords.append(kw)
                seen.add(kw)
        # ç„¶åæ·»åŠ åŸå§‹å…³é”®è¯
        for kw in extracted_keywords:
            if kw not in seen:
                merged_keywords.append(kw)
                seen.add(kw)
        extracted_keywords = merged_keywords
    
    debug_print(f"ã€åˆå¹¶å…³é”®è¯ã€‘{extracted_keywords}")
    
    # éªŒè¯æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œé¿å…ç©ºæŸ¥è¯¢æˆ–æœ‰é—®é¢˜çš„æŸ¥è¯¢
    if not search_text or search_text.strip() == "" or len(search_text.strip()) < 2:
        search_text = "ç®—æ³• é¢˜ç›®"
        debug_print(f"ã€æŸ¥è¯¢ä¿®æ­£ã€‘ä½¿ç”¨é»˜è®¤æŸ¥è¯¢: {search_text}")
    
    # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæŸ¥è¯¢å­—ç¬¦ä¸²åŒ…å«å¯èƒ½å¯¼è‡´Luceneé—®é¢˜çš„å­—ç¬¦ï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨æŸ¥è¯¢
    lucene_problem_chars = ['[', ']', '(', ')', '{', '}', '<', '>', '"', "'"]
    has_lucene_issues = any(char in search_text for char in lucene_problem_chars)
    
    if has_lucene_issues:
        debug_print(f"ã€æŸ¥è¯¢ç­–ç•¥ã€‘æ£€æµ‹åˆ°ç‰¹æ®Šå­—ç¬¦ï¼Œç›´æ¥ä½¿ç”¨å¤‡ç”¨æŸ¥è¯¢")
        use_backup_directly = True
    else:
        use_backup_directly = False
    
    # å¦‚æœæ²¡æœ‰Luceneé—®é¢˜ï¼Œå°è¯•ä¸»æŸ¥è¯¢
    rows = []
    if not use_backup_directly:
        try:
            with driver.session(database=settings.database.neo4j_database) as sess:
                recs = sess.run(CYPHER_ENHANCED_HYBRID, {
                    "q": search_text,  # ä½¿ç”¨å¤„ç†åçš„æœç´¢æ–‡æœ¬
                    "qvec": qvec,
                    "keywords": extracted_keywords
                })
                rows = [r.data() for r in recs]
                if rows:
                    debug_print("âœ… ä¸»æŸ¥è¯¢æˆåŠŸ")
        except Exception as e:
            debug_print(f"âš ï¸ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼š{e}")
            debug_print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æŸ¥è¯¢ç­–ç•¥...")
            use_backup_directly = True
    
    # ä½¿ç”¨å¤‡ç”¨æŸ¥è¯¢
    if use_backup_directly or not rows:
        try:
            with driver.session(database=settings.database.neo4j_database) as sess:
                backup_query = """
                MATCH (a:Algorithm)
                WITH a, 
                     CASE 
                       WHEN size($keywords) = 0 THEN 0.1
                       WHEN any(keyword IN $keywords WHERE toLower(a.title) CONTAINS toLower(keyword)) THEN 0.8
                       WHEN any(keyword IN $keywords WHERE any(alias IN a.aliases WHERE toLower(alias) CONTAINS toLower(keyword))) THEN 0.6
                       WHEN any(keyword IN $keywords WHERE any(k IN a.keywords WHERE toLower(k) CONTAINS toLower(keyword))) THEN 0.4
                       ELSE 0.1
                     END AS score
                WHERE score > 0.1
                WITH a, score
                ORDER BY score DESC
                LIMIT 5
                
                OPTIONAL MATCH (a)-[:PREREQUISITE]->(p:Concept)
                OPTIONAL MATCH (a)-[:APPLICATION]->(u:Concept)
                OPTIONAL MATCH (a)-[:PITFALL]->(f:Concept)
                OPTIONAL MATCH (a)-[:HAS_EXAMPLE]->(e:Example)
                OPTIONAL MATCH (a)-[:HAS_CHUNK]->(c:Chunk)
                
                // è·å–è¯¦ç»†çš„ç¤ºä¾‹ä»£ç ä¿¡æ¯ï¼ˆå¤‡ç”¨æŸ¥è¯¢ï¼‰
                WITH a, score,
                     collect(DISTINCT p.name)[0..8] AS prereq,
                     collect(DISTINCT u.name)[0..8] AS apps,
                     collect(DISTINCT f.name)[0..8] AS pitfalls,
                     collect(DISTINCT e)[0..8] AS all_examples,
                     [c IN collect(DISTINCT c)[0..5] | c.content] AS snippets

                // æ„å»ºè¯¦ç»†çš„ç¤ºä¾‹ä¿¡æ¯ï¼ŒåªåŒ…å«ç¡®å®å­˜åœ¨çš„å±æ€§ï¼ˆå¤‡ç”¨æŸ¥è¯¢ï¼‰
                WITH a, score, prereq, apps, pitfalls, snippets,
                     [ex IN all_examples | {
                         id: ex.id,
                         title: ex.title,
                         description: ex.description,
                         code: ex.code,
                         solution: ex.solution
                     }] AS detailed_examples
                
                RETURN a.uid AS uid, a.title AS title,
                       a.principle AS principle, a.cpx_time AS time, a.cpx_space AS space,
                       a.intro AS intro, a.keywords AS keywords, a.aliases AS aliases,
                       prereq, apps, pitfalls, detailed_examples, snippets, score AS fused, 
                       [{route: 'keyword', s: score}] AS contribs
                """
                
                recs = sess.run(backup_query, {"keywords": extracted_keywords})
                rows = [r.data() for r in recs]
                
                if rows:
                    debug_print("âœ… å¤‡ç”¨æŸ¥è¯¢æˆåŠŸ")
                else:
                    debug_print("âš ï¸ å¤‡ç”¨æŸ¥è¯¢ä¹Ÿæœªæ‰¾åˆ°ç»“æœï¼Œä½¿ç”¨é»˜è®¤ç®—æ³•ä¿¡æ¯")
                    
        except Exception as backup_error:
            debug_print(f"âŒ å¤‡ç”¨æŸ¥è¯¢ä¹Ÿå¤±è´¥ï¼š{backup_error}")
            rows = []
    
    # å¦‚æœæ‰€æœ‰æŸ¥è¯¢éƒ½å¤±è´¥ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ç»“æœä½†è®©ç³»ç»Ÿç»§ç»­è¿è¡Œ
    if not rows:
        debug_print("ğŸ”§ æ‰€æœ‰æŸ¥è¯¢éƒ½å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºç¡€ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”")
    
    debug_print(f"ã€æ£€ç´¢ç»“æœã€‘æ‰¾åˆ° {len(rows)} ä¸ªç›¸å…³ç®—æ³•")
    for r in rows:
        routes = [c["route"] for c in (r.get("contribs") or [])]
        debug_print(f"[å€™é€‰] {r['title']} | fused={r['fused']:.4f} | routes={routes}")
    
    return rows

